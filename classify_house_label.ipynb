{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MaxAbsScaler, TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, HistGradientBoostingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from src.utils.processor.dataframe_selector import DataFrameSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(clfs: dict, clf_name:str, perf: dict, acc: float, metric: str):\n",
    "    if clf_name in clfs:\n",
    "        if clfs[clf_name].get(metric) < acc:\n",
    "            clfs[clf_name].update(perf)\n",
    "    else: \n",
    "        clfs[clf_name] = perf\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'housing_price_clf'\n",
    "\n",
    "DATA_DIR = pathlib.Path('.', 'data', PROJECT_NAME)\n",
    "DATA_PATH = list(DATA_DIR.glob('train.csv'))\n",
    "\n",
    "# load data\n",
    "house_df = pd.read_csv(DATA_PATH[0], index_col=0) \n",
    "\n",
    "# drop any duplicated data\n",
    "house_df.drop(columns=[\"Id\", \"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\", \"MoSold\", \"YrSold\"], inplace=True)\n",
    "house_df.drop_duplicates(inplace=True)\n",
    "house_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test data\n",
    "Goal is predicting whether a house is, 1 == expensive or 0 == not expensive and should therefor be set as y variable. \n",
    "\n",
    "the test data size we'll use is 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data distribution\n",
    "house_df['Expensive'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the value count we conclude that the data is imbalanced and take care to make a stratified sampling when making the train and test split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data and label\n",
    "labels = house_df.pop('Expensive')\n",
    "features = house_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test data size\n",
    "test_size = .2\n",
    "\n",
    "# split data into train and test\n",
    "X, X_test, y, y_test = train_test_split(features, labels, test_size=test_size, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make num_pipeline\n",
    "num_cols = X.select_dtypes(exclude=['object', 'category']).columns\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('selector', DataFrameSelector(feature_names=num_cols)),\n",
    "    ('imputer', KNNImputer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make categorical pipeline\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('selector', DataFrameSelector(feature_names=cat_col)),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='N_A')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build preprocessor pipe\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num_pipe', num_pipe, make_column_selector(dtype_include='number')), \n",
    "    ('target_pipe', cat_pipe, make_column_selector(dtype_include=['object', 'category']))\n",
    "])#.set_output(transform=\"pandas\")\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelling\n",
    "We'll be training several models, test different settings and save the best performant model. Best performant model is the model with highest f1-score. We use f1-score here because our data is unbalanced towards not expensive (85.1%) but as this is a binary classification example, accuracy would work just as fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameter dict\n",
    "prep_param_grid = {\n",
    "    \"input__preprocessor__num_pipe__imputer__n_neighbors\": range(2,10),\n",
    "    \"input__preprocessor__num_pipe__imputer__weights\": ['uniform', 'distance'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of scalers we want to test\n",
    "scalers ={\n",
    "    'no_scaler' : {},\n",
    "    'standard_scaler' : {\n",
    "        'scl' : StandardScaler(),\n",
    "        'param_gird': {\n",
    "            \"input__scaler__with_mean\": [True, False],\n",
    "            \"input__scaler__with_std\": [True, False],\n",
    "        }\n",
    "    },\n",
    "    'standard_scaler' : {\n",
    "        'scl' : MaxAbsScaler(),\n",
    "        'param_gird': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of classifiers we want to test\n",
    "classifiers ={\n",
    "    'decision_tree' : {\n",
    "        'clf' : DecisionTreeClassifier(),\n",
    "        'param_grid': {\n",
    "            'clf__max_depth': range(2, 100, 2),\n",
    "            'clf__min_samples_split': range(2, 20),\n",
    "            \"clf__min_samples_leaf\": range(3, 12, 2),\n",
    "            'clf__criterion':['gini', 'entropy'],\n",
    "        }\n",
    "    },\n",
    "    'linear' : {\n",
    "        'clf' : SGDClassifier(),\n",
    "        'param_grid': {\n",
    "            'clf__loss': ['hinge', 'log_loss', 'modified_huber'],\n",
    "        }\n",
    "    },\n",
    "    'random_forest' : {\n",
    "        'clf' : RandomForestClassifier(),\n",
    "        'param_grid': {\n",
    "            'clf__max_depth': range(2, 100, 2),\n",
    "            'clf__min_samples_split': range(2, 20),\n",
    "            \"clf__min_samples_leaf\": range(3, 12, 2),\n",
    "            'clf__criterion':['gini', 'entropy'],\n",
    "        }\n",
    "    },\n",
    "    'regression' : {\n",
    "        'clf' : LogisticRegression(max_iter=10000),\n",
    "        'param_grid': {\n",
    "            'clf__solver': ['liblinear'],\n",
    "            'clf__penalty': ['l1', 'l2'],\n",
    "        }\n",
    "    },\n",
    "    'hist_gradient_booster' : {\n",
    "        'clf' : HistGradientBoostingClassifier(),\n",
    "        'param_grid': {\n",
    "            'clf__learning_rate': [x / 1000 for x in range(100, 1, -1)],\n",
    "        }\n",
    "    },\n",
    "    'gradient_booster' : {\n",
    "        'clf' : GradientBoostingClassifier(),\n",
    "        'param_grid': {\n",
    "            'clf__learning_rate': [x / 1000 for x in range(100, 1, -1)],\n",
    "            'clf__n_estimators': range(100,1000,100)\n",
    "        }\n",
    "    },\n",
    "     'ada_booster' : {\n",
    "        'clf' : AdaBoostClassifier(),\n",
    "        'param_grid': {\n",
    "            'clf__learning_rate': [x / 1000 for x in range(100, 1, -1)]\n",
    "        }\n",
    "    },\n",
    "    'xg_boost' : {\n",
    "        'clf': XGBClassifier(objective='binary:logistic', tree_method='hist', device='cpu'),\n",
    "        'param_grid': {\n",
    "            \"clf__n_estimators\": range(100,1000,100),\n",
    "            \"clf__learning_rate\":  [x / 1000 for x in range(100, 1, -1)],\n",
    "            \"clf__max_depth\": range(2, 14, 2),\n",
    "            \"clf__min_child_weight\": range(1, 8, 2)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dict to store the best classifiers of each type\n",
    "best_classifiers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for best performing setting for each classifier\n",
    "for clf_name, clf_params in tqdm(classifiers.items()):        \n",
    "    for scaler, scl_param in tqdm(scalers.items(), desc=clf_name):\n",
    "        # build input pipeline\n",
    "        input_pipe = Pipeline([('preprocessor', preprocessor)])\n",
    "        if scl_param:\n",
    "            input_pipe = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('scaler', scl_param.get('scl', None))\n",
    "            ])\n",
    "\n",
    "        # build clf pipeline\n",
    "        clf_pipeline = Pipeline([\n",
    "            ('input', input_pipe), \n",
    "            ('clf', clf_params.get('clf', None)),\n",
    "        ])\n",
    "    \n",
    "        # build param_grid\n",
    "        param_grid = {}\n",
    "        for d in [prep_param_grid, scl_param.get('param_grid'), clf_params.get('param_grid')]:\n",
    "            if d:\n",
    "                param_grid.update(d)\n",
    "\n",
    "        # grid search best parameters\n",
    "        grid_search = RandomizedSearchCV(\n",
    "            estimator=clf_pipeline,\n",
    "            param_distributions=param_grid,\n",
    "            cv=5,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=0,\n",
    "            n_iter=500\n",
    "        )\n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        # store best estimator\n",
    "        acc = accuracy_score(y_test, grid_search.predict(X_test))\n",
    "        perf = {\n",
    "            'clf': grid_search.best_estimator_,\n",
    "            'acc': acc,\n",
    "            'f1': grid_search.best_score_,\n",
    "            'clf_pipeline': clf_pipeline,\n",
    "        }\n",
    "        best_classifiers = update_dict(best_classifiers, clf_name, perf, acc, metric='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_name, clf_perf in best_classifiers.items():\n",
    "    print(f'{clf_name}:\\n - acc-score: {clf_perf.get(\"acc\")};\\n - f1-score: {clf_perf.get(\"f1\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier([(clf_name, clf.get('clf')) for clf_name, clf in best_classifiers.items()], voting='hard')\n",
    "voting_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Prediction accuracy on the test data is: {accuracy_score(y_test, voting_clf.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path('.', 'data')\n",
    "DATA_PATH = list(DATA_DIR.glob('test.csv'))\n",
    "\n",
    "# load data\n",
    "test_df = pd.read_csv(DATA_PATH[0]) \n",
    "\n",
    "test_id = test_df.pop('Id')\n",
    "test_features = test_df\n",
    "\n",
    "test_result = pd.DataFrame({\n",
    "    'Id':test_id,\n",
    "    'Expensive' : voting_clf.predict(test_features)\n",
    "})\n",
    "\n",
    "test_result.to_csv('test_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
